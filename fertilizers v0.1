{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91717,"databundleVersionId":12184666,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport tensorflow.keras.layers as layers\nfrom tqdm import tqdm # for progress bar\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# read csv\n\ncompetition_file_path = \"/kaggle/input/playground-series-s5e6\"\ntrain_file_path = competition_file_path + \"/train.csv\"\ntest_file_path = competition_file_path + \"/test.csv\"\nsample_subbission_file_path = competition_file_path + \"/sample_submission.csv\"\n\n\ntrain_dataset = pd.read_csv(train_file_path)\ntest_dataset = pd.read_csv(test_file_path)\nsample_submission = pd.read_csv(sample_subbission_file_path)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Investigate dataset","metadata":{}},{"cell_type":"code","source":"train_dataset.head()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset.info()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Looking at the data, we can see that there are no nulls so we don't have to worry about that\n\n\nBut we'll have to find a good way to hot encode the training dataset","metadata":{}},{"cell_type":"markdown","source":"## Encoding data","metadata":{}},{"cell_type":"code","source":"### soil type\n\nprint(train_dataset[\"Soil Type\"].describe())\nprint(train_dataset[\"Soil Type\"].unique())","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since there are 5 unique soil types in this column, we can easily encode this 0 -> 4","metadata":{}},{"cell_type":"code","source":"# encoding crop type\n\nprint(train_dataset[\"Crop Type\"].describe())\nprint(train_dataset[\"Crop Type\"].unique())","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Similary, there are 11 unique crop types in this column, we can easily encode this 0 -> 10","metadata":{}},{"cell_type":"code","source":"# encoding crop type\n\nprint(train_dataset[\"Fertilizer Name\"].describe())\nprint(train_dataset[\"Fertilizer Name\"].unique())","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Similary, there are 7 unique crop types in this column, we can easily encode this 0 -> 6","metadata":{}},{"cell_type":"markdown","source":"## Enscapluting logic","metadata":{}},{"cell_type":"code","source":"class model_manager:\n    model = 0\n    def __init__(self):\n        self.define_simple_dnn_model()\n        \n    def define_simple_dnn_model(self):\n        self.model = tf.keras.models.Sequential([\n            layers.Input([9]),\n            layers.Dense(16 ),\n            layers.Dense(8),\n            layers.Dense(4),\n            layers.Dense(2),\n            layers.Dense(7),\n        ])\n\nmodel_manager_instance = model_manager()\nmodel_manager_instance.model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class workflow:\n    train_dataset = []\n    test_dataset = []\n    sample_submission = []\n    input_to_model = []\n    ground_truth = []\n    model_prediction = []\n    model = 0\n    model_prediction_decoded = []\n    fertilizer_encodings = {\n            0:\"28-28\",\n            1:\"17-17-17\",\n            2:\"10-26-26\",\n            3:\"DAP\",\n            4:\"20-20\",\n            5:\"14-35-14\",\n            6:\"Urea\"\n        }\n    max_data_row = 250\n\n    def __init__(self, model_manager):\n        self.load_data()\n        self.model = model_manager.model\n\n    def load_data(self):\n        # read csv\n        competition_file_path = \"/kaggle/input/playground-series-s5e6\"\n        train_file_path = competition_file_path + \"/train.csv\"\n        test_file_path = competition_file_path + \"/test.csv\"\n        sample_subbission_file_path = competition_file_path + \"/sample_submission.csv\"\n        \n        self.train_dataset = pd.read_csv(train_file_path)\n        self.test_dataset = pd.read_csv(test_file_path)\n        self.sample_submission = pd.read_csv(sample_subbission_file_path)\n\n    def prepare_datasets(self):\n        self.reduce_dataset()\n        self.encode_soil_type(self.train_dataset, is_printing = False)\n        self.encode_soil_type(self.test_dataset)\n\n        self.encode_crop_type(self.train_dataset)\n        self.encode_crop_type(self.test_dataset)\n\n        self.encode_fertilizer_name(self.train_dataset)\n\n        self.train_dataset = self.convert_column_type(self.train_dataset)\n        self.test_dataset = self.convert_column_type(self.test_dataset)\n\n        self.ground_truth = self.train_dataset.pop(\"Fertilizer Name\")\n        self.input_to_model = self.train_dataset\n        self.convert_ground_truth_to_categories_optimised()\n\n        #self.ground_truth = self.convert_column_type(self.ground_truth)\n        #self.input_to_model = self.convert_column_type(self.input_to_model)\n\n    def reduce_dataset(self):\n        self.train_dataset = self.train_dataset[0:self.max_data_row]\n        self.test_dataset = self.test_dataset[0:self.max_data_row]\n\n    def encode_soil_type(self, dataset, is_printing = False):\n        if is_printing:\n            print(dataset[\"Soil Type\"].describe())\n            print(dataset[\"Soil Type\"].unique())\n\n        self.encode_value(dataset, \"Soil Type\", \"Clayey\", 0)\n        self.encode_value(dataset, \"Soil Type\", \"Sandy\", 1)\n        self.encode_value(dataset, \"Soil Type\", \"Red\", 0)\n        self.encode_value(dataset, \"Soil Type\", \"Loamy\", 3)\n        self.encode_value(dataset, \"Soil Type\", \"Black\", 4)\n\n    def encode_value(self, dataset, column_name, value_to_replace, new_value):\n        column = dataset.loc[:, [column_name]]\n        mask = [dataset[column_name] == value_to_replace]\n        column[mask] = new_value\n        dataset[column_name] = column\n\n    def encode_crop_type(self, dataset):\n        self.encode_value(dataset, \"Crop Type\", \"Sugarcane\", 0)\n        self.encode_value(dataset, \"Crop Type\", \"Millets\", 1)\n        self.encode_value(dataset, \"Crop Type\", \"Barley\", 2)\n        self.encode_value(dataset, \"Crop Type\", \"Paddy\", 3)\n        self.encode_value(dataset, \"Crop Type\", \"Pulses\", 4)\n        self.encode_value(dataset, \"Crop Type\", \"Tobacco\", 5)\n        self.encode_value(dataset, \"Crop Type\", \"Ground Nuts\", 6)\n        self.encode_value(dataset, \"Crop Type\", \"Maize\", 7)\n        self.encode_value(dataset, \"Crop Type\", \"Cotton\", 8)\n        self.encode_value(dataset, \"Crop Type\", \"Wheat\", 9)\n        self.encode_value(dataset, \"Crop Type\", \"Oil seeds\", 10)\n\n    def encode_fertilizer_name(self, dataset):\n        self.encode_value(dataset, \"Fertilizer Name\", \"28-28\", 0)\n        self.encode_value(dataset, \"Fertilizer Name\", \"17-17-17\", 1)\n        self.encode_value(dataset, \"Fertilizer Name\", \"10-26-26\", 2)\n        self.encode_value(dataset, \"Fertilizer Name\", \"DAP\", 3)\n        self.encode_value(dataset, \"Fertilizer Name\", \"20-20\", 4)\n        self.encode_value(dataset, \"Fertilizer Name\", \"14-35-14\", 5)\n        self.encode_value(dataset, \"Fertilizer Name\", \"Urea\", 6)\n\n    def convert_database_types(self, database, is_test_dataset = False):\n        self.convert_column_type(self.train_dataset[\"Soil Type\"])\n        self.convert_column_type(self.train_dataset[\"Crop Type\"])\n        self.convert_column_type(self.train_dataset[\"Fertilizer Name\"])\n        \n    def convert_column_type(self, column):\n        return column.astype(\"int64\")\n\n    def convert_ground_truth_to_categories_optimised(self):\n        indices_to_change = np.arange(0, self.max_data_row, 1)\n        categorised_ground_truth = np.array([[0,0,0,0,0,0,0] for x in indices_to_change])\n        for index in tqdm(indices_to_change):\n            category_value = np.zeros(7)\n            category_value[self.ground_truth[index]] = 1\n            category_value_to_add = np.array([category_value])\n            categorised_ground_truth[index] = category_value_to_add    \n        self.ground_truth = categorised_ground_truth\n    \n    def build_model(self):\n        loss_function = tf.keras.losses.BinaryCrossentropy()\n        self.model.compile(optimizer=\"adam\", loss=loss_function, metrics=[\"accuracy\"])\n\n    def train_model(self):\n        self.model.fit(self.input_to_model, self.ground_truth)\n\n    def predict_test_dataset(self):\n        self.model_prediction = self.model.predict(self.test_dataset)\n    \n    def decode_model_predictions(self):\n        find_indices_of_highest_values_in_model_prediction()\n        decode_class_numbers_into_fertilizer_names()\n\n        def find_indices_of_highest_values_in_model_prediction(self):\n            model_prediction_decoded = np.array([[0,0,0] for x in self.model_prediction])\n            for class_index, prediction in enumerate(tqdm(self.model_prediction)):\n                top_3_predictions = sorted(prediction, reverse=True)[:3]\n                highest_classes = np.zeros(3)\n                for index, value in enumerate(top_3_predictions):\n                    class_encoding = np.where(value == prediction)[0]\n                    highest_classes[index] = class_encoding\n                model_prediction_decoded[class_index] = highest_classes\n            self.model_prediction_decoded = model_prediction_decoded\n    \n        def decode_class_numbers_into_fertilizer_names(self):\n            ## The max number of characters for each value in the array is defined by the initalised value here\n            ## so I made the initalised value the longest possible value\n            model_prediction_decoded = np.array([\"xx-xx-xx xx-xx-xx xx-xx-xx\" for x in self.model_prediction])\n            for class_index, prediction in enumerate(tqdm(self.model_prediction_decoded)):\n                fertizier_prediction = self.decode_fertilizer_list(prediction)\n                model_prediction_decoded[class_index] = fertizier_prediction\n            self.model_prediction_decoded = model_prediction_decoded\n    \n        def decode_fertilizer(self, class_number):\n            return self.fertilizer_encodings.get(class_number)\n    \n        def decode_fertilizer_list(self, array_of_class_numbers):\n            array_to_return = np.zeros(3, dtype=\"object\")\n            for index, value in enumerate(array_of_class_numbers):\n                array_to_return[index] = self.decode_fertilizer(value)\n            array_as_string_to_return = self.convert_list_into_single_string(array_to_return)\n            return array_as_string_to_return\n    \n        def convert_list_into_single_string(self, input_list, delimiter = \" \"):\n            return delimiter.join(input_list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"workflow_instance = workflow(model_manager_instance)\nworkflow_instance.prepare_datasets()\nworkflow_instance.build_model()\nworkflow_instance.train_model()\nworkflow_instance.predict_test_dataset()\nworkflow_instance.decode_model_predictions()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(np.shape(workflow_instance.model_prediction_decoded))\nworkflow_instance.model_prediction_decoded","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"workflow_instance.model_prediction_decoded","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(np.shape(workflow_instance.ground_truth))\nworkflow_instance.ground_truth[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}